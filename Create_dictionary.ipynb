{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Create_dictionary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yardenadika/Generative_Deep_Learning/blob/main/Create_dictionary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqNf0gFo4bSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5777dd-d1af-44c9-d4ea-a737d3d7ecca"
      },
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from pprint import pprint\n",
        "\n",
        "# How to create a dictionary from a list of sentences?\n",
        "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
        "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
        "             \"interrogation that went wrong, one that was intended to lead\", \n",
        "             \"to his abduction from Turkey, according to two sources.\"]\n",
        "\n",
        "documents_2 = [\"One source says the report will likely conclude that\", \n",
        "                \"the operation was carried out without clearance and\", \n",
        "                \"transparency and that those involved will be held\", \n",
        "                \"responsible. One of the sources acknowledged that the\", \n",
        "                \"report is still being prepared and cautioned that\", \n",
        "                \"things could change.\"]\n",
        "\n",
        "# Tokenize(split) the sentences into words\n",
        "texts = [[text for text in doc.split()] for doc in documents]\n",
        "\n",
        "# Create dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "# Get information about the dictionary\n",
        "print(dictionary)\n",
        "#> Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I_-5q2d4bSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21e245f-720f-4b9f-8cdd-c38be711b966"
      },
      "source": [
        "# Show the word to id map\n",
        "print(dictionary.token2id)\n",
        "\n",
        "#> {'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, \n",
        "#> 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \n",
        "#> \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, \n",
        "#> 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, \n",
        "#> 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, \n",
        "#> 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, \n",
        "#> 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgPHzyaL4bSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f078bd8-5b1b-401d-f2a2-4dc84fca8a1c"
      },
      "source": [
        "documents_3 = [\"The intersection graph of paths in trees\",\n",
        "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
        "               \"Graph minors A survey\"]\n",
        "\n",
        "texts_3 = [[text for text in doc.split()] for doc in documents_3]\n",
        "\n",
        "dictionary.add_documents(texts_3)\n",
        "\n",
        "\n",
        "# Now, the dictionary should have been updated with the new words (tokens).\n",
        "print(dictionary)\n",
        "#> Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
        "\n",
        "print(dictionary.token2id)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
            "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'graph': 33, 'in': 34, 'intersection': 35, 'paths': 36, 'trees': 37, 'Graph': 38, 'IV': 39, 'Widths': 40, 'and': 41, 'minors': 42, 'ordering': 43, 'quasi': 44, 'well': 45, 'A': 46, 'survey': 47}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVpN9WBe4bSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "185741e9-85ab-4b95-ff30-f525a02c9da4"
      },
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# List with 2 sentences\n",
        "my_docs = [\"Who let the dogs out?\", \"Who? Who? Who? Who?\"]\n",
        "# Tokenize the docs\n",
        "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
        "# Create the Corpus\n",
        "mydict = corpora.Dictionary()\n",
        "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
        "pprint(mycorpus)\n",
        "#> [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8FYAod44bSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c074a3a5-d934-413d-f3b2-5e5444ba583a"
      },
      "source": [
        "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n",
        "pprint(word_counts)\n",
        "#> [[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]\n"
          ]
        }
      ]
    }
  ]
}